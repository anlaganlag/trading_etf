from __future__ import print_function, absolute_import
from gm.api import *
import pandas as pd
import numpy as np
import os
import json
from datetime import datetime, timedelta
from dotenv import load_dotenv
from config import config

load_dotenv()

TOP_N = 4
REBALANCE_PERIOD_T = 10
STOP_LOSS = 0.30          # æ­¢æŸ 30%
TRAILING_TRIGGER = 0.15   # 15% å¼€å¯è¿½è¸ªæ­¢ç›ˆ
TRAILING_DROP = 0.03      # å›è½ 3% æ­¢ç›ˆé€€å‡º

# Account ID for Live Trading
ACCOUNT_ID = os.environ.get('GM_ACCOUNT_ID', '658419cf-ffe1-11f0-a908-00163e022aa6')

# TOP_N = 4
# REBALANCE_PERIOD_T = 10
# STOP_LOSS = 0.20
# TRAILING_TRIGGER = 0.15
# EBALANCE_PERIOD_T = 10
# STOP_LOSS = 0.20
# TRAILING_TRIGGER = 0.15
# TRAILING_DROP = 0.05

# åŸæ­¢æŸæ­¢ç›ˆå‚æ•°
# STOP_LOSS = 0.05  # æ­¢æŸ
# TRAILING_TRIGGER = 0.06 # æ­¢ç›ˆ
# TRAILING_DROP = 0.02  # æ­¢ç›ˆå›è½



# --- åŸå§‹å‚æ•°  ---
# TOP_N = 5
# REBALANCE_PERIOD_T = 13
# STOP_LOSS = 0.20  # æ­¢æŸ
# TRAILING_TRIGGER = 0.10 # æ­¢ç›ˆ
# TRAILING_DROP = 0.05  # æ­¢ç›ˆå›è½



# START_DATE = os.environ.get('GM_START_DATE', '2021-12-03 09:00:00')
# END_DATE = os.environ.get('GM_END_DATE', '2026-01-23 16:00:00')


START_DATE='2021-12-03 09:00:00'
END_DATE='2026-01-23 16:00:00'

# START_DATE='2024-09-01 09:00:00'
# END_DATE='2026-01-23 16:00:00'

# START_DATE='2021-12-03 09:00:00'
# END_DATE='2026-01-23 16:00:00'
DYNAMIC_POSITION = True # å¼€å¯åŠ¨æ€ä»“ä½
ENABLE_META_GATE = True # False=å¹½çµæ¨¡å¼(åªè®°å½•ä¸å‡ä»“ï¼Œæ”¶ç›Šé«˜) | True=å¼€å¯é˜²å¾¡(å›æ’¤å°)


# === è¯„åˆ†æœºåˆ¶å¼€å…³ ===
SCORING_METHOD = 'SMOOTH' # 'STEP': åŸç‰ˆç¡¬æˆªæ–­(å‰15æ»¡åˆ†) | 'SMOOTH': çº¿æ€§è¡°å‡(å‰30å¹³æ»‘)

# === ä¸»é¢˜é›†ä¸­åº¦æ§åˆ¶ ===
MAX_PER_THEME = 2  # åŒä¸€ä¸»é¢˜æœ€å¤šå…¥é€‰å‡ åªï¼ˆé˜²æ­¢æ¿å—è¿‡åº¦é›†ä¸­ï¼‰è®¾ä¸º0ä¸é™åˆ¶

# === å®è§‚é£æ§åŸºå‡†é…ç½® ===
# æ²ªæ·±300: 'SHSE.510300' | åˆ›ä¸šæ¿æŒ‡: 'SZSE.159915'
MACRO_BENCHMARK = 'SHSE.510300' 

# === çŠ¶æ€æ–‡ä»¶ ===
STATE_FILE = "rolling_state_simple.json"

# === å®ç›˜æ•°æ®æ›´æ–° ===
LIVE_DATA_UPDATE = False  # True=æ¯æ—¥æ›´æ–°prices_dfï¼ˆå®ç›˜å¿…å¼€ï¼‰| False=åªç”¨initæ•°æ®ï¼ˆå›æµ‹ï¼‰


MIN_SCORE = 20







class Tranche:
    def __init__(self, t_id, initial_cash=0):
        self.id = t_id
        self.cash = initial_cash
        self.holdings = {} # {symbol: shares}
        self.pos_records = {} # {symbol: {'entry_price': x, 'high_price': y}}
        self.total_value = initial_cash
        self.guard_triggered_today = False 

    def to_dict(self):
        return self.__dict__

    @staticmethod
    def from_dict(d):
        t = Tranche(d["id"], d["cash"])
        t.holdings = d["holdings"]
        t.pos_records = d["pos_records"]
        t.total_value = d["total_value"]
        # guard_triggered_today doesn't need persistence, resets daily
        return t

    def update_value(self, price_map):
        val = self.cash
        current_symbols = list(self.holdings.keys())
        for sym in current_symbols:
            if sym in price_map:
                price = price_map[sym]
                val += self.holdings[sym] * price
                if sym in self.pos_records:
                    self.pos_records[sym]['high_price'] = max(self.pos_records[sym]['high_price'], price)
        self.total_value = val

    def check_guard(self, price_map):
        to_sell = []
        is_tp = False
        for sym, rec in self.pos_records.items():
            if sym not in self.holdings: continue
            curr_price = price_map.get(sym, 0)
            if curr_price <= 0: continue

            entry, high = rec['entry_price'], rec['high_price']
            
            # Stop Loss OR Trailing Take Profit
            if (curr_price < entry * (1 - STOP_LOSS)) or \
               (high > entry * (1 + TRAILING_TRIGGER) and curr_price < high * (1 - TRAILING_DROP)):
                to_sell.append(sym)
                if curr_price >= entry: is_tp = True

        return to_sell, is_tp

    def sell(self, symbol, price):
        if symbol in self.holdings:
            shares = self.holdings[symbol]
            self.cash += shares * price
            del self.holdings[symbol]
            if symbol in self.pos_records: del self.pos_records[symbol]

    def buy(self, symbol, cash_allocated, price):
        if price <= 0: return
        shares = int(cash_allocated / price / 100) * 100
        cost = shares * price
        if shares > 0 and self.cash >= cost:
            self.cash -= cost
            self.holdings[symbol] = self.holdings.get(symbol, 0) + shares
            self.pos_records[symbol] = {'entry_price': price, 'high_price': price}

class RollingPortfolioManager:
    def __init__(self):
        self.tranches = []
        self.params = {"T": REBALANCE_PERIOD_T, "top_n": TOP_N}
        self.initialized = False
        self.days_count = 0 
        self.state_path = os.path.join(config.BASE_DIR, STATE_FILE)
        self.nav_history = []  # Track daily virtual NAV (T-Close Valuation)
        
    def load_state(self):
        if os.path.exists(self.state_path):
            try:
                with open(self.state_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.params = data.get("params", self.params)
                    self.initialized = data.get("initialized", False)
                    self.days_count = data.get("days_count", 0)  # Load persisted day count
                    self.tranches = [Tranche.from_dict(d) for d in data.get("tranches", [])]
                print(f"âœ“ Loaded State: {len(self.tranches)} tranches, Day {self.days_count} from {self.state_path}")
                return True
            except Exception as e:
                print(f"âš ï¸ Failed to load state: {e}")
                print(f"   Will initialize fresh state...")
        return False
        
    def save_state(self):
        data = {
            "params": self.params,
            "initialized": self.initialized,
            "days_count": self.days_count, # Persist day count
            "tranches": [t.to_dict() for t in self.tranches]
        }
        try:
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å†™å…¥ï¼Œç„¶åé‡å‘½åï¼ˆåŸå­æ“ä½œï¼‰
            temp_path = self.state_path + '.tmp'
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2)
            # åŸå­æ›¿æ¢
            if os.path.exists(self.state_path):
                os.remove(self.state_path)
            os.rename(temp_path, self.state_path)
        except Exception as e:
            print(f"âš ï¸ Failed to save state: {e}")
            print(f"   State path: {self.state_path}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç­–ç•¥ç»§ç»­è¿è¡Œ

    def initialize_tranches(self, total_cash):
        if self.initialized and self.tranches: return
        share = total_cash / REBALANCE_PERIOD_T  # Aggressive Allocation (1/7th instead of 1/10th)
        self.tranches = [Tranche(i, share) for i in range(self.params["T"])]
        self.initialized = True
        print(f"Initialized {self.params['T']} tranches.")
        self.save_state()

    def reconcile_with_broker(self, real_positions):
        """
        Reconcile virtual tranches with actual broker positions.
        If Virtual > Real (Phantom Holdings), remove from virtual and refund cash.
        If Virtual < Real (Unmanaged), the Sync logic later handles selling.
        """
        # 1. Sum up all virtual holdings
        virtual_map = {} # sym -> total_shares
        for t in self.tranches:
            for sym, shares in t.holdings.items():
                virtual_map[sym] = virtual_map.get(sym, 0) + shares
        
        # 2. Compare with Real
        for sym, v_qty in virtual_map.items():
            r_qty = real_positions.get(sym, 0)
            diff = v_qty - r_qty
            
            if diff > 0: # Phantom: Virtual has 1000, Real has 0. need to remove 1000.
                print(f"âš ï¸ Reconcile: Found {diff} phantom shares of {sym} (Real {r_qty} vs Virtual {v_qty}). Fixing...")
                remaining_to_remove = diff
                
                # Deduct from tranches (FIFO by tranche order)
                for t in self.tranches:
                    if sym in t.holdings:
                        has_qty = t.holdings[sym]
                        remove_qty = min(has_qty, remaining_to_remove)
                        
                        if remove_qty > 0:
                            # 1. Update Holdings
                            t.holdings[sym] -= remove_qty
                            if t.holdings[sym] == 0:
                                del t.holdings[sym]
                            
                            # 2. Refund Cash (using entry price)
                            if sym in t.pos_records:
                                entry_p = t.pos_records[sym]['entry_price']
                                refund_val = remove_qty * entry_p
                                t.cash += refund_val
                                # Clean up record if full removal
                                if sym not in t.holdings:
                                    del t.pos_records[sym]
                                    
                            print(f"   -> Tranche {t.id}: Removed {remove_qty}, Refunded {refund_val:.2f}")
                            
                            remaining_to_remove -= remove_qty
                            if remaining_to_remove <= 0:
                                break

def init(context):
    print(f"Initializing Simple Strategy (T={REBALANCE_PERIOD_T}, TopN={TOP_N}, Mode={SCORING_METHOD})")
    context.rpm = RollingPortfolioManager()
    
    # Check RUN_MODE from env (default to LIVE if not set, to be safe? No, default backtest)
    run_mode = os.environ.get('GM_MODE', 'BACKTEST').upper()
    context.mode = MODE_BACKTEST if run_mode == 'BACKTEST' else MODE_LIVE
    context.account_id = ACCOUNT_ID
    
    # --- Meta-Gate State Machine (Capital Layer) ---
    context.market_state = 'SAFE' # SAFE, CAUTION, DANGER
    context.risk_scaler = 1.0     # 1.0, 0.5, 0.0
    context.br_history = []       # For smoothing Broken_Ratio (Rolling 3 days)
    
    # Meta-Gate Thresholds (Hysteresis)
    # Meta-Gate Thresholds (Hysteresis) - Firefighter Mode V2
    # Strategy: 1.0 (Safe/Low Caution) -> 0.7 (Pre-Danger) -> 0.0 (Danger)
    context.BR_CAUTION_IN = 0.40  
    context.BR_CAUTION_OUT = 0.30 
    context.BR_DANGER_IN = 0.60   
    context.BR_DANGER_OUT = 0.50
    context.BR_PRE_DANGER = 0.55  # New Buffer Threshold
    
    # 1. Load Whitelist & Theme Map
    excel_path = os.path.join(config.BASE_DIR, "ETFåˆå¹¶ç­›é€‰ç»“æœ.xlsx")
    df_excel = pd.read_excel(excel_path)
    df_excel.columns = df_excel.columns.str.strip()
    rename_map = {'symbol': 'etf_code', 'sec_name': 'etf_name', 'name_cleaned': 'theme'}
    df_excel = df_excel.rename(columns=rename_map)
    if 'theme' not in df_excel.columns: df_excel['theme'] = df_excel['etf_name']
    context.whitelist = set(df_excel['etf_code'])
    context.theme_map = df_excel.set_index('etf_code')['theme'].to_dict()

    # --- INJECT MISSING TICKERS (Monkey Patch) ---
    # These tickers were found in the winning transaction logs but missing from the excel
    missing_tickers = [
        '560860', '516650', '513690', '159516', '159995', 
        '517520', '512400', '159378', '159638', '516150', 
        '515400', '159852', '159599', '159998'
    ]
    print(f"Injecting {len(missing_tickers)} missing tickers into whitelist...")
    for code in missing_tickers:
        full_code = f"SHSE.{code}" if code.startswith('5') else f"SZSE.{code}"
        context.whitelist.add(full_code)
        if full_code not in context.theme_map:
            context.theme_map[full_code] = 'Injected_Alpha'
    # ---------------------------------------------


    # 2. Build Price Matrix
    # 2. Build Price Matrix & Load HS300
    if context.mode == MODE_LIVE:
        print("â˜ï¸ Live Mode: Fetching history from GM API (Last 260 days)...")
        
        # --- A. è·å–æ ‡çš„è¡Œæƒ… (Batch) ---
        all_symbols = list(context.whitelist)
        end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        start_time = (datetime.now() - timedelta(days=400)).strftime('%Y-%m-%d %H:%M:%S')
        
        symbol_str = ",".join(all_symbols)
        try:
            hd = history(symbol=symbol_str, frequency='1d', start_time=start_time, end_time=end_time, fields='symbol,close,eob', fill_missing='last', adjust=ADJUST_PREV, df=True)
            if not hd.empty:
                hd['eob'] = pd.to_datetime(hd['eob']).dt.tz_localize(None)
                context.prices_df = hd.pivot(index='eob', columns='symbol', values='close').ffill()
                
                # è·å–æ­¤æ—¶æ­¤åˆ»çš„æœ€æ–°ä»·æ ¼å¹¶æ’å…¥/æ›´æ–°åˆ°æœ€åä¸€è¡Œ
                current_data = current(symbols=symbol_str)
                now_prices = {item['symbol']: item['price'] for item in current_data}
                today_dt = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
                context.prices_df.loc[today_dt] = pd.Series(now_prices)
                context.prices_df = context.prices_df.ffill()
                
                print(f"â˜ï¸ Live Data Ready: {context.prices_df.shape} (Includes today's live tick)")
            else:
                print("âš ï¸ Warning: API returned empty history data!")
                context.prices_df = pd.DataFrame()
        except Exception as e:
            print(f"âš ï¸ Error fetching live data: {e}")
            context.prices_df = pd.DataFrame()

        # --- B. è·å–å®è§‚åŸºå‡†è¡Œæƒ… (Macro) ---
        try:
            bm_hd = history(symbol=MACRO_BENCHMARK, frequency='1d', start_time=start_time, end_time=end_time, fields='close,eob', fill_missing='last', adjust=ADJUST_PREV, df=True)
            if not bm_hd.empty:
                bm_hd['eob'] = pd.to_datetime(bm_hd['eob']).dt.tz_localize(None)
                context.benchmark_df = bm_hd.set_index('eob')['close'].sort_index()
                
                # æ’å…¥å½“å‰ä»·æ ¼
                bm_current = current(symbols=MACRO_BENCHMARK)
                if bm_current:
                    context.benchmark_df.loc[today_dt] = bm_current[0]['price']
                print(f"Benchmark {MACRO_BENCHMARK} Loaded: {len(context.benchmark_df)} days (API).")
            else:
                context.benchmark_df = None
                print(f"Warning: Benchmark {MACRO_BENCHMARK} API data empty.")
        except Exception as e:
            context.benchmark_df = None
            print(f"Warning: Failed to fetch Benchmark {MACRO_BENCHMARK} API: {e}")

    else:
        # Backtest Mode: Force API Usage (Consistent with Step 2 Logic)
        print("ğŸ“‰ Backtest Mode: Fetching history from GM API (Ensuring consistency)...")
        
        # Calculate time range: Need buffer because Strategy needs MA250 etc.
        # Fetch 1 year data BEFORE backtest start date.
        start_dt = pd.Timestamp(START_DATE) - timedelta(days=365)
        start_time = start_dt.strftime('%Y-%m-%d %H:%M:%S')
        end_time = END_DATE
        
        print(f"   Fetching Data Range: {start_time} -> {end_time}")
        
        # --- A. è·å–æ ‡çš„è¡Œæƒ… (Batch) ---
        all_symbols = list(context.whitelist)
        symbol_str = ",".join(all_symbols)
        
        try:
            # Note: For large backtests, if symbol_str is huge, this might need chunking.
            # Assuming whitelist size is reasonable (<200).
            hd = history(symbol=symbol_str, frequency='1d', start_time=start_time, end_time=end_time, fields='symbol,close,eob', fill_missing='last', adjust=ADJUST_PREV, df=True)
            
            if not hd.empty:
                hd['eob'] = pd.to_datetime(hd['eob']).dt.tz_localize(None)
                context.prices_df = hd.pivot(index='eob', columns='symbol', values='close').ffill()
                print(f"âœ“ Data Ready: {context.prices_df.shape} (API)")
            else:
                print("âš ï¸ Critical: API returned empty history data for backtest!")
                context.prices_df = pd.DataFrame()
        except Exception as e:
            print(f"âš ï¸ Error fetching backtest data from API: {e}")
            context.prices_df = pd.DataFrame()

        # --- B. è·å–å®è§‚åŸºå‡†è¡Œæƒ… (Macro) ---
        try:
            bm_hd = history(symbol=MACRO_BENCHMARK, frequency='1d', start_time=start_time, end_time=end_time, fields='close,eob', fill_missing='last', adjust=ADJUST_PREV, df=True)
            if not bm_hd.empty:
                bm_hd['eob'] = pd.to_datetime(bm_hd['eob']).dt.tz_localize(None)
                context.benchmark_df = bm_hd.set_index('eob')['close'].sort_index()
                print(f"âœ“ Benchmark {MACRO_BENCHMARK} Review: {len(context.benchmark_df)} days (API).")
            else:
                context.benchmark_df = None
                print(f"Warning: Benchmark {MACRO_BENCHMARK} API data empty.")
        except Exception as e:
            context.benchmark_df = None
            print(f"Warning: Failed to fetch Benchmark {MACRO_BENCHMARK} from API: {e}")
            
        # === CRITICAL SAFETY CHECK ===
        # If Benchmark is missing, the Macro-Gate will fail (defaulting to 1.0 exposure),
        # causing a massive 40%+ drawdown. We must STOP if this happens.
        if context.benchmark_df is None or context.benchmark_df.empty:
            raise ValueError(f"âŒ CRITICAL: Benchmark {MACRO_BENCHMARK} (Macro Shield) Data is MISSING! Strategies cannot run safely without it. Check API/Internet.")
        else:
             print(f"âœ… Macro Shield Active: {MACRO_BENCHMARK} Data Loaded ({len(context.benchmark_df)} days)")

    # 3. State Management
    if context.mode == MODE_BACKTEST and os.path.exists(context.rpm.state_path): 
        try:
            os.remove(context.rpm.state_path)
            print("ğŸ—‘ï¸ Backtest Mode: Deleted previous state file.", flush=True)
        except Exception as e:
            print(f"âš ï¸ Failed to delete state file: {e}", flush=True)
        context.rpm.load_state() 
    else:
        context.rpm.load_state()

    # context.days_count moved to rpm.days_count for persistence
    # è®¢é˜…æŒ‡æ•°è¡Œæƒ…ç”¨äºå®æ—¶æ›´æ–°ï¼ˆå¯é€‰ï¼‰
    subscribe(symbols='SHSE.000001', frequency='1d')
    
    # === å®šæ—¶ä»»åŠ¡ ===
    # æ¯å¤© 14:55 æ‰§è¡Œç­–ç•¥é€»è¾‘
    schedule(schedule_func=algo, date_rule='1d', time_rule='14:55:00')



def get_market_regime(context, current_dt):
    """åˆ¤æ–­å¸‚åœºç¯å¢ƒï¼šè¿”å›ä»“ä½ç³»æ•° 0.5-1.0
    ä»…ä½¿ç”¨å¾®è§‚ETFå¸‚åœºå¹¿åº¦ï¼Œä¸ä½¿ç”¨å®è§‚å¹´çº¿ï¼ˆé¿å…ç‰›å¸‚è¸ç©ºï¼‰
    """
    history = context.prices_df[context.prices_df.index <= current_dt]
    if len(history) < 60: return 1.0
    
    # === å®è§‚é£æ§ (Macro Filter) ===
    # ä½¿ç”¨åŸºå‡†æŒ‡æ•° (å¦‚æ²ªæ·±300) çš„åŠå¹´çº¿ (MA120) ä½œä¸ºç‰›ç†Šåˆ†ç•Œ
    macro_multiplier = 1.0
    debug_msg = ""
    
    if context.benchmark_df is not None:
        # Pre-process: Ensure tz-naive
        if context.benchmark_df.index.tz is not None:
             context.benchmark_df.index = context.benchmark_df.index.tz_localize(None)
             
        bm_hist = context.benchmark_df[context.benchmark_df.index <= current_dt]
        
        if len(bm_hist) > 120:
            current_price = bm_hist.iloc[-1]
            ma120 = bm_hist.tail(120).mean()
            
            # --- DEBUG BLOCK FOR 2022-2024 CRASH ---
            # Print status every Monday (to reduce log spam) or if Multiplier changes
            is_monday = current_dt.weekday() == 0
            
            if current_price < ma120:
                macro_multiplier = 0.5 # ç†Šå¸‚
                if is_monday: debug_msg = f"[MACRO: BEAR] {MACRO_BENCHMARK} Price {current_price:.2f} < MA120 {ma120:.2f} -> Scale 0.5"
            else:
                if is_monday: debug_msg = f"[MACRO: BULL] {MACRO_BENCHMARK} Price {current_price:.2f} > MA120 {ma120:.2f} -> Scale 1.0"
                
    if debug_msg: print(f"{current_dt.date()} {debug_msg}")
    
    # === å¾®è§‚å¼ºåº¦: ETFå¸‚åœºå¹¿åº¦ ===
    recent = history.tail(60)
    ma20 = recent.tail(20).mean()
    ma60 = recent.mean()
    current = recent.iloc[-1]
    above_ma20 = (current > ma20).sum() / len(current)
    above_ma60 = (current > ma60).sum() / len(current)
    strength = (above_ma20 + above_ma60) / 2

    # åŸºç¡€ä»“ä½é€»è¾‘
    if strength > 0.6: base_pos = 1.0
    elif strength > 0.4: base_pos = 0.9
    else: base_pos = 0.3

    # æœ€ç»ˆä»“ä½ = å¾®è§‚ä»“ä½ * å®è§‚æŠ˜æ‰£
    final_pos = base_pos * macro_multiplier
    
    # Turbo Logic: å¦‚æœæ˜¯ç†Šå¸‚(Macro<1)ä¸”å¾®è§‚å¼±åŠ¿(Strength<=0.4)ï¼Œç›´æ¥ç©ºä»“é˜²å¾¡
    if macro_multiplier < 1.0 and strength <= 0.4:
        return 0.0

    return final_pos

def get_ranking(context, current_dt):
    history = context.prices_df[context.prices_df.index <= current_dt]
    if len(history) < 251: return None, None

    last_row = history.iloc[-1]
    base_scores = pd.Series(0.0, index=history.columns)
    
    # Updated Optimal Weights (Decoupled Logic)
    # Updated Optimal Weights (Decoupled Logic)
    # R1=30, R3=-70, R5=0 (Linear 'weight' is 0, but we will use it as a Gate), R20=150
    periods_rule = {1: 30, 3: -70, 5: 0, 10: 0, 20: 150}

    rets_dict = {}
    r5_raw = None # Capture R5 constraints
    
    for p, pts in periods_rule.items():
        # è¿™é‡Œä½¿ç”¨ç»å¯¹æ¶¨å¹…ï¼Œä¸å¯¹æ¯” HS300
        rets = (last_row / history.iloc[-(p+1)]) - 1
        rets_dict[f'r{p}'] = rets
        
        if p == 5:
            r5_raw = rets 

        # ç›´æ¥æŒ‰æ”¶ç›Šæ’å
        ranks = rets.rank(ascending=False, method='min')
        
        # Skip calculation if weight is 0
        if pts != 0:
            if SCORING_METHOD == 'SMOOTH':
                decay = (30 - ranks) / 30
                decay = decay.clip(lower=0)
                base_scores += decay * pts
            else: 
                base_scores += (ranks <= 15) * pts
    
    # --- Structural Gate (Non-linear Filter) ---
    # Upgrade: Dynamic Volatility Gate (Z-Score)
    # Instead of fixed -8%, check if drop exceeds k * sigma.
    # Logic: Structure is BROKEN if the drop is statistically abnormal (e.g. > 2 sigma).
    
    # --- Structural Gate (Non-linear Filter) ---
    # Upgrade: Dynamic Volatility Gate (Z-Score) with ROBUST Ruler
    # Ruler = Lagged Downside Volatility
    # 1. Lagged: Use t-65 to t-5 (Pre-crash volatility) to avoid "Adaptive Failure"
    # 2. Downside: Only measure downside risk to avoid punishing upside volatility
    
    daily_rets = history.pct_change()
    
    # Lagged slice: Exclude last 5 days from the ruler
    lagged_rets = daily_rets.iloc[:-5].tail(60) 
    
    # Downside only: Measure std dev of negative returns
    downside_rets = lagged_rets[lagged_rets < 0]
    
    # Calculate per-symbol metrics
    vol_down = downside_rets.std()
    vol_full = lagged_rets.std()
    count_down = downside_rets.count()
    
    # Vectorized fallback: Use downside vol if > 10 points, else full vol
    # Note: vol_down or vol_full can be NaN if column is empty
    vol_ruler = vol_down.where(count_down > 10, vol_full)
    
    # Fill remaining NaNs and apply floor
    vol_ruler = vol_ruler.fillna(0.01)
    vol_ruler = vol_ruler.clip(lower=0.005)

    # 2. Calculate Z-Score of the 5-day return
    # Expected 5-day vol = daily_vol * sqrt(5)
    expected_5d_vol = vol_ruler * np.sqrt(5)
    r5_z_score = r5_raw / expected_5d_vol
    
    # 3. Dynamic Gate Thresholds (Split Micro/Macro)
    # K_ENTRY: Micro Gate (Individual stock filtering) - Default 1.6
    # K_CRASH: Macro Gate (Systemic failure detection) - Default 2.5
    k_entry = float(os.environ.get('OPT_R5_K', 1.6)) 
    k_crash = float(os.environ.get('OPT_K_CRASH', 2.5))
    
    # --- META-GATE: Broken Ratio Calculation (Capital Layer) ---
    # Calculate how many "trees are falling" in the forest
    if r5_z_score is not None:
        # Filter Z-Scores to Whitelist (Market Universe)
        universe_z = r5_z_score[r5_z_score.index.isin(context.whitelist)].dropna()
        
        if len(universe_z) >= 20: # Min Sample Size to avoid noise
            # Count broken structures using K_CRASH (Systemic Fire)
            broken_count = (universe_z < -k_crash).sum()
            br_raw = broken_count / len(universe_z)
            
            # Smooth BR (Mean of last 3 days)
            context.br_history.append(br_raw)
            if len(context.br_history) > 3: context.br_history.pop(0)
            br_smooth = np.mean(context.br_history)
            
            # --- V3 Upgrade: Dynamic Threshold (Breadth + Depth) ---
            # If Market Depth is bad (Median Z < -2.3), lower the Danger threshold.
            median_z = np.median(universe_z)
            effective_danger_in = context.BR_DANGER_IN # Default 0.60
            if median_z < -2.3:
                effective_danger_in = 0.50 # Less aggressive penalty (was 0.40)
            
            # 1. State Machine Transition (Hysteresis Logic)
            prev_state = context.market_state
            
            if context.market_state == 'SAFE':
                if br_smooth > context.BR_CAUTION_IN:
                    context.market_state = 'CAUTION'
            elif context.market_state == 'CAUTION':
                if br_smooth > effective_danger_in: # Dynamic: 0.60 or 0.40
                    context.market_state = 'DANGER'
                elif br_smooth < context.BR_CAUTION_OUT:
                    context.market_state = 'SAFE'
            elif context.market_state == 'DANGER':
                if br_smooth < context.BR_DANGER_OUT:
                    context.market_state = 'CAUTION'
            
            # 2. Assign Risk Scaler (Action Mapping)
            # Firefighter V2: Non-linear escalation
            if context.market_state == 'SAFE':
                context.risk_scaler = 1.0
            elif context.market_state == 'CAUTION':
                 # Buffer Zone: If approaching 60%, cut exposure to 70%
                 if br_smooth >= context.BR_PRE_DANGER:
                     context.risk_scaler = 0.7 
                 else:
                     context.risk_scaler = 1.0 # Ignore "Noise" (<55%)
            elif context.market_state == 'DANGER':
                context.risk_scaler = 0.0 # Shutdown
            
            # LOG DATA for Analysis: Date, BR_Raw, BR_Smooth, State, Risk_Scaler
            # Tag: METAGATE_LOG
            print(f"METAGATE_LOG,{current_dt},{br_raw:.4f},{br_smooth:.4f},{context.market_state},{context.risk_scaler}")

            if context.market_state != prev_state:
                print(f"[{current_dt}] ğŸš¦ METAGATE: {prev_state} -> {context.market_state} (BR={br_smooth:.1%}, Scaler={context.risk_scaler})")

    # --- Individual Gate ---
    is_structure_intact = pd.Series(True, index=base_scores.index)
    if k_entry > 0 and r5_raw is not None:
         # "Gate": Keep only if r5 z-score > -k_entry (Filtering weak stocks)
         is_structure_intact = r5_z_score > -k_entry

    # Apply Gate: Zero out scores for broken structures
    base_scores = base_scores * is_structure_intact.astype(float)

    # 2. é™åˆ¶åœ¨ç™½åå•å†…
    valid_scores = base_scores[base_scores.index.isin(context.whitelist)]
    
    # 3. åŸºç¡€å¾—åˆ†é˜ˆå€¼
    valid_scores = valid_scores[valid_scores >= MIN_SCORE]
    
    if valid_scores.empty: return None, base_scores

    data_to_df = {
        'score': valid_scores, 
        'theme': [context.theme_map.get(c, 'Unknown') for c in valid_scores.index],
        'etf_code': valid_scores.index 
    }
    
    for p in periods_rule.keys():
        data_to_df[f'r{p}'] = rets_dict[f'r{p}'][valid_scores.index]

    df = pd.DataFrame(data_to_df)
    
    # è¿˜åŸæ’åºé€»è¾‘ï¼šScore -> r1 (çŸ­åŠ¨é‡) -> å…¶ä»–å‘¨æœŸ -> Code
    sort_cols = ['score', 'r1', 'r3', 'r5', 'r10', 'r20', 'etf_code']
    asc_order = [False, False, False, False, False, False, True]
    
    return df.sort_values(by=sort_cols, ascending=asc_order), base_scores



    # V6.1 Score Logic: Module 1 (Relative Alpha) + Module 2 (Trend Filter)
    history = context.prices_df[context.prices_df.index <= current_dt]
    if len(history) < 251: return None, None

    last_row = history.iloc[-1]
    
    # === Module 2: è¶‹åŠ¿è¿‡æ»¤ (Trend Filter) ===
    # æ ¸å¿ƒé€»è¾‘ï¼šåªæœ‰å¤„äºâ€œå¯è¶‹åŠ¿åŒºâ€çš„æ ‡çš„æ‰å‚ä¸è¯„åˆ†
    ma20 = history.tail(20).mean()
    ma60 = history.tail(60).mean()
    # åˆ¤æ–­ï¼šä»·æ ¼åœ¨20æ—¥å‡çº¿ä¸Šæ–¹ï¼ˆçŸ­æœŸèµ°å¼ºï¼‰ä¸”ä¸å¤„äºä¸¥é‡çš„é•¿æœŸç ´ä½ï¼ˆä»·æ ¼>MA60æˆ–MA20>MA60ï¼‰
    is_trending = (last_row > ma20) & (last_row > ma60)
    
    # --- Module 1: ç›¸å¯¹å¼ºåº¦æ¨¡å— (Relative Alpha) ---
    # è·å–åŒæœŸçš„å®è§‚åŸºå‡†è¡¨ç°ä½œä¸ºåŸºå‡†
    bm_hist = None
    if context.benchmark_df is not None:
        bm_hist = context.benchmark_df[context.benchmark_df.index <= current_dt]

    base_scores = pd.Series(0.0, index=history.columns)
    # æ¿€è¿›ç‰ˆæƒé‡ï¼šä¿æŒåŸæœ‰çš„ Inverse Middle é€»è¾‘
    periods_rule = {1: 50, 3: -70, 5: -70, 10: 0, 20: 150}
    
    rets_dict = {}
    for p, pts in periods_rule.items():
        # è®¡ç®—ç»å¯¹æ¶¨å¹…
        rets = (last_row / history.iloc[-(p+1)]) - 1
        rets_dict[f'r{p}'] = rets
        
        # è®¡ç®— Alpha (è¶…é¢æ”¶ç›Š)
        if bm_hist is not None and len(bm_hist) > p:
            bm_p_ret = (bm_hist.iloc[-1] / bm_hist.iloc[-(p+1)]) - 1
            alpha = rets - bm_p_ret
        else:
            alpha = rets # é™çº§ä¸ºç»å¯¹æ”¶ç›Š
            
        # åŸºäº Alpha è¿›è¡Œæ’å
        ranks = alpha.rank(ascending=False, method='min')
        
        if SCORING_METHOD == 'SMOOTH':
             decay = (30 - ranks) / 30
             decay = decay.clip(lower=0)
             base_scores += decay * pts
        else: # 'STEP' åŸç‰ˆ
             base_scores += (ranks <= 15) * pts
    
    # --- æœ€ç»ˆæ•´åˆè¿‡æ»¤ ---
    # 1. åº”ç”¨è¶‹åŠ¿è¿‡æ»¤ (Module 2)
    # è¶‹åŠ¿ä¸å¥½çš„æ ‡çš„å¾—åˆ†ç›´æ¥æ¸…é›¶ï¼Œä¸å‚ä¸åç»­ TopN é€‰æ‹”
    base_scores = base_scores * is_trending.astype(float)
    
    # 2. é™åˆ¶åœ¨ç™½åå•å†…
    valid_scores = base_scores[base_scores.index.isin(context.whitelist)]
    
    # 3. åŸºç¡€å¾—åˆ†é˜ˆå€¼
    valid_scores = valid_scores[valid_scores >= MIN_SCORE]
    
    if valid_scores.empty: return None, base_scores

    # æ„å»ºç»“æœ DataFrame ç”¨äºæ’åº
    # å³ä½¿è¯„åˆ†ä¸€æ ·ï¼Œæˆ‘ä»¬ä¹Ÿä¼˜å…ˆé€‰ç»å¯¹æ”¶ç›Šæœ€å¥½çš„æ ‡çš„æˆ–è€…æ˜¯ä»£ç æ›´è€ƒå‰çš„ä»¥ä¿è¯ç¡®å®šæ€§
    data_to_df = {
        'score': valid_scores, 
        'theme': [context.theme_map.get(c, 'Unknown') for c in valid_scores.index],
        'etf_code': valid_scores.index 
    }
    
    for p in periods_rule.keys():
        data_to_df[f'r{p}'] = rets_dict[f'r{p}'][valid_scores.index]

    df = pd.DataFrame(data_to_df)
    
    # æ’åºï¼šå¾—åˆ† -> 20æ—¥ç»å¯¹æ”¶ç›Š -> 1æ—¥æ”¶ç›Š -> ä»£ç 
    sort_cols = ['score', 'r20', 'r1', 'etf_code']
    asc_order = [False, False, False, True]
    
    return df.sort_values(by=sort_cols, ascending=asc_order), base_scores

# def on_bar(context, bars): -> Renamed to algo
def algo(context):
    current_dt = context.now.replace(tzinfo=None) # Scheduled func uses context.now
    
    # === å®ç›˜æ¨¡å¼ï¼šæ³¨å…¥å®æ—¶è¡Œæƒ… ===
    if context.mode == MODE_LIVE:
        try:
            # è·å–ç™½åå•å†…æ‰€æœ‰æ ‡çš„çš„æœ€æ–° tick
            ticks = current(symbols=list(context.whitelist))
            today_date = current_dt.replace(hour=0, minute=0, second=0, microsecond=0)
            
            # æ„å»ºä»Šæ—¥æ•°æ®å­—å…¸
            today_data = {tick['symbol']: tick['price'] for tick in ticks if tick['price'] > 0}
            
            if today_data:
                # è½¬æ¢ä¸º DataFrame è¡Œå¹¶è¿½åŠ /æ›´æ–°
                # æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†æ€§èƒ½ï¼Œç®€å•å¤„ç†ã€‚å¦‚æœæ•°æ®é‡å·¨å¤§éœ€ä¼˜åŒ–ã€‚
                today_series = pd.Series(today_data, name=today_date)
                
                # å¦‚æœä»Šå¤©å·²ç»å­˜åœ¨ï¼ˆæ¯”å¦‚é‡å¤è¿è¡Œï¼‰ï¼Œåˆ™æ›´æ–°ï¼›å¦åˆ™è¿½åŠ 
                if today_date in context.prices_df.index:
                    context.prices_df.loc[today_date, today_series.index] = today_series
                else:
                    # ä½¿ç”¨ concat è¿½åŠ 
                    context.prices_df = pd.concat([context.prices_df, today_series.to_frame().T])
                
                context.prices_df.sort_index(inplace=True)
                print(f"â˜ï¸ Real-time Data Injected: {len(today_data)} symbols at {current_dt}")
        except Exception as e:
            print(f"âš ï¸ Failed to fetch real-time data: {e}")

    context.rpm.days_count += 1
    # Save immediately to record the day increment
    context.rpm.save_state()

    # 1. Init if needed
    if not context.rpm.initialized:
        account = context.account()
        if account is None:
             print("âš ï¸ Account data not ready yet. Skipping init...")
             return
             
        cash = account.cash.available if hasattr(account.cash, 'available') else account.cash.nav
        context.rpm.initialize_tranches(cash)

    # === Reconcile Virtual vs Real ===
    # ä¿®å¤ï¼šå¼ºè¡Œå¯¹é½è™šæ‹Ÿåˆ†ä»“ä¸çœŸå®æŒä»“ï¼Œé˜²æ­¢â€œå¹½çµæŒä»“â€å¯¼è‡´åç»­é€»è¾‘é”™ä¹±
    try:
        real_positions = {p.symbol: p.amount for p in context.account().positions()}
        context.rpm.reconcile_with_broker(real_positions)
    except Exception as e:
        print(f"âš ï¸ Reconcile Error: {e}")

    # 2. Get Prices (Fixed: use history slicing to avoid looking into the future)
    # This ensures we only see data UP TO and INCLUDING 'today' (T day)
    history_until_now = context.prices_df[context.prices_df.index <= current_dt]
    if history_until_now.empty:
        return
    today_prices = history_until_now.iloc[-1]
    price_map = today_prices.to_dict()

    # 3. Update All Tranches (Value & Guard Check)
    # Using T-day closing prices for accounting
    for t in context.rpm.tranches:
        t.update_value(price_map)
        to_sell, _ = t.check_guard(price_map)
        if to_sell:
            t.guard_triggered_today = True
            print(f"{current_dt} | Tranche {t.id} Guard Triggered: {to_sell}")
            for sym in to_sell: 
                t.sell(sym, price_map.get(sym, 0))
        else:
            t.guard_triggered_today = False

    # 4. Rolling Rebalance (Buy/Sell)
    # Identify which tranche is rotating today
    active_idx = (context.rpm.days_count - 1) % REBALANCE_PERIOD_T
    active_tranche = context.rpm.tranches[active_idx]
    
    # Sell Old Holdings in the active tranche
    for sym in list(active_tranche.holdings.keys()):
        price = price_map.get(sym, 0)
        if price > 0: 
            active_tranche.sell(sym, price)
    
    # 5. Buy New Holdings (based on T-day ranking)
    ranking_df, _ = get_ranking(context, current_dt)
    
    if ranking_df is not None and not active_tranche.guard_triggered_today:
        # Theme-based filtering
        if MAX_PER_THEME > 0:
            targets = []
            theme_count = {}
            for code, row in ranking_df.iterrows():
                theme = row['theme']
                if theme_count.get(theme, 0) < MAX_PER_THEME:
                    targets.append(code)
                    theme_count[theme] = theme_count.get(theme, 0) + 1
                if len(targets) >= TOP_N:
                    break
        else:
            targets = ranking_df.head(TOP_N).index.tolist()

        if targets:
            # --- æ–¹æ¡ˆ E: åˆ†æœŸä¸“æ¬¾ + 1% æ‘©æ“¦ç¼“å†² ---
            # ä½¿ç”¨è¯¥åˆ†ä»“å†…éƒ¨çš„ç°é‡‘ï¼Œå¹¶ç•™å‡º 1% ç¼“å†²åº”å¯¹æ‘©æ“¦ï¼ˆæ»‘ç‚¹ã€ç¨è´¹ã€èˆå…¥ï¼‰
            usable_cash = active_tranche.cash * 0.99
            
            # Position Sizing
            # 1. Existing Dynamic Position (Trend)
            regime_scale = 1.0
            if DYNAMIC_POSITION:
                regime_scale = get_market_regime(context, current_dt)
            
            # 2. Meta-Gate Risk Scaler (Broken Ratio)
            meta_scale = 1.0
            if ENABLE_META_GATE:
                meta_scale = getattr(context, 'risk_scaler', 1.0)
            
            # Combined
            final_scale = regime_scale * meta_scale
            allocate_cash = usable_cash * final_scale
            
            # Logging / Monitoring
            current_scaler = getattr(context, 'risk_scaler', 1.0)
            if current_scaler < 1.0:
                if ENABLE_META_GATE:
                     print(f"   ğŸ›¡ï¸ Risk Control: Allocation x {meta_scale:.1f} (Meta-Gate) -> Final {final_scale:.1%}")
                else:
                     print(f"   ğŸ›¡ï¸ [Ghost Mode] Meta-Gate signaled {current_scaler:.1f}, but ignored for performance.")
            
            # --- AGGRESSIVE CLAMPING REMOVED ---
            # Trust the internal ledger (active_tranche.cash) because sells will settle.
            # Only log a warning if actual cash is low, but do not block.
            avail = context.account().cash.available if hasattr(context.account().cash, 'available') else context.account().cash.nav
            if allocate_cash > avail:
                print(f"âš ï¸ Value Warning: Internal Cash {allocate_cash:.0f} > Broker Available {avail:.0f}")
                print(f"   Assuming funds from today's sells will be available for buys.")
                # allocate_cash = avail  <-- THIS LINE CAUSED THE BUG


            # per_amt = allocate_cash / len(targets)
            # Use Unequal Weighting (Top 3 gets 2x)
            # Targets are already sorted by Rank (Head).
            # If N=6. Weights = [2, 2, 2, 1, 1, 1] => Sum 9.
            # If Top 3 is 'Better', this should help.
            n_targets = len(targets)
            weights = []
            for i in range(n_targets):
                if i < 3: weights.append(2)
                else: weights.append(1)
            
            total_weight = sum(weights)
            unit_val = allocate_cash / total_weight
            
            for idx, sym in enumerate(targets):
                w = weights[idx]
                amt = unit_val * w
                active_tranche.buy(sym, amt, price_map.get(sym, 0))
    
    active_tranche.update_value(price_map)

    # 6. Synchronize Internal Bookkeeping with Broker
    # Since it's 15:00, orders will be queued for T+1 Open execution
    global_tgt = {}
    for t in context.rpm.tranches:
        for sym, shares in t.holdings.items():
            global_tgt[sym] = global_tgt.get(sym, 0) + shares
            
    # Get current actual positions from broker
    real_positions = {p['symbol']: p['amount'] for p in context.account().positions()}
    
    # Execute Sells first to free up capital/slots
    # Execute Sells first to free up capital/slots
    # Minimal Safe Sell Logic (Iterate ALL broker positions)
    # ç§»é™¤ç™½åå•é™åˆ¶ï¼Œç¡®ä¿èƒ½å–å‡ºæ‰€æœ‰éç›®æ ‡æŒä»“
    for pos in context.account().positions():
        sym = pos.symbol
        tgt = global_tgt.get(sym, 0)
        diff = pos.amount - tgt
        
        if diff > 0:
            # Check T+0 availability
            if pos.available > 0:
                qty_to_sell = min(diff, pos.available)
                vol = int(qty_to_sell)
                if vol > 0:
                    order_volume(symbol=sym, volume=vol, side=OrderSide_Sell, order_type=OrderType_Market, position_effect=PositionEffect_Close)
                    print(f"ğŸ“‰ Selling {sym}: {vol} (Target {tgt}, Held {pos.amount})", flush=True)
            else:
                 print(f"ğŸ”’ Skip Sell {sym}: Want to sell {diff} but available is 0 (T+1 Lock)", flush=True)

    # Execute Buys
    # Refetch actual positions after sends (though they might not be filled yet, wait logic is complex, 
    # so we trust the 'available' cash check will handle subsequent buys)
    real_positions_map = {p.symbol: p.amount for p in context.account().positions()}
    
    for sym, target_amt in global_tgt.items():
        current_amt = real_positions_map.get(sym, 0)
        if current_amt < target_amt:
            # FIX: target_volume expects int, cast to int
            tgt_vol = int(target_amt)
            order_target_volume(symbol=sym, volume=tgt_vol, order_type=OrderType_Market, position_side=PositionSide_Long)

    context.rpm.save_state()
    
    # 7. Record Virtual NAV (Simulating T-Close execution)
    total_equity = sum(t.total_value for t in context.rpm.tranches)
    context.rpm.nav_history.append(total_equity)

def on_backtest_finished(context, indicator):
    print(f"\n=== GM STANDARD REPORT (T+1 EXECUTION) ===")
    print(f"Return: {indicator.get('pnl_ratio', 0)*100:.2f}%")
    print(f"Max DD: {indicator.get('max_drawdown', 0)*100:.2f}%")
    print(f"Sharpe: {indicator.get('sharp_ratio', 0):.2f}")
    
    # Calculate Simulated Performance (T-Close Execution)
    # history = context.rpm.nav_history
    # if history:
    #     nav = pd.Series(history)
    #     if nav.iloc[0] > 0:
    #         ret = (nav.iloc[-1] / nav.iloc[0] - 1) * 100
    #         dd = ((nav - nav.cummax()) / nav.cummax()).min() * 100
    #         daily_ret = nav.pct_change().dropna()
    #         sharpe = np.sqrt(252) * daily_ret.mean() / daily_ret.std() if daily_ret.std() > 0 else 0
            
    #         print(f"\n=== SIMULATED REPORT (T-CLOSE EXECUTION / LIVE PROXY) ===")
    #         print(f"Return: {ret:.2f}%")
    #         print(f"Max DD: {dd:.2f}%")
    #         print(f"Sharpe: {sharpe:.2f}")
    #         print("(Note: This matches run_optimization results and Live Trading logic)")
    
    # print("\nrolling0")
if __name__ == '__main__':
    # === è¿è¡Œæ¨¡å¼é…ç½® ===
    # 'BACKTEST': å›æµ‹æ¨¡å¼ (è·‘å†å²æ•°æ®)
    # 'LIVE': å®ç›˜/ä»¿çœŸæ¨¡å¼ (è¿æ¥ç»ˆç«¯å®æ—¶äº¤æ˜“)
    RUN_MODE = 'BACKTEST' 

    # ç­–ç•¥ ID (è¯·ç¡®ä¿ä¸æ˜é‡‘ç»ˆç«¯é‡Œçš„ç­–ç•¥ ID ä¸€è‡´)
    STRATEGY_ID = 'aea75195-00dd-11f1-866a-00ffda9d6e63'
    if RUN_MODE == 'LIVE':
        print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ä»¿çœŸ/å®ç›˜äº¤æ˜“...")
        print(f"âš ï¸ è¯·ç¡®è®¤å·²åœ¨æ˜é‡‘ç»ˆç«¯å°†è´¦æˆ· [658419cf-ffe1-11f0-a908-00163e022aa6] ç»‘å®šåˆ°ç­–ç•¥ [{STRATEGY_ID}]")
        
        run(strategy_id=STRATEGY_ID, 
            filename='gm_strategy_rolling0.py', 
            mode=MODE_LIVE,
            token=os.getenv('MY_QUANT_TGM_TOKEN'))
            
    else:
        print(f"ğŸ“‰ æ­£åœ¨å¯åŠ¨å›æµ‹...")
        run(strategy_id=STRATEGY_ID, 
            filename='gm_strategy_rolling0.py', 
            mode=MODE_BACKTEST,
            token=os.getenv('MY_QUANT_TGM_TOKEN'), 
            backtest_start_time=START_DATE, 
            backtest_end_time=END_DATE,
            backtest_adjust=ADJUST_PREV, 
            backtest_initial_cash=1000000,
            backtest_commission_ratio=0.0001)
