# AIä¼˜åŒ–å¤šå‘¨æœŸé€‰è‚¡æ¨¡å‹æŠ€æœ¯æ–¹æ¡ˆ

## ä¸€ã€é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡

### 1.1 å½“å‰é—®é¢˜
ç°æœ‰çš„é€‰è‚¡æ¨¡å‹é‡‡ç”¨å›ºå®šæƒé‡çš„å¤šå› å­æ‰“åˆ†ï¼š
- 20æ—¥æ”¶ç›Šï¼šæƒé‡ 150
- 1æ—¥æ”¶ç›Šï¼šæƒé‡ 30
- 3æ—¥æ”¶ç›Šï¼šæƒé‡ -70

è¿™äº›æƒé‡å‚æ•°æ˜¯**äººå·¥è®¾å®š**çš„ï¼Œç¼ºä¹æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–ä¾æ®ã€‚

### 1.2 ä¼˜åŒ–ç›®æ ‡
æ„å»ºä¸€ä¸ªAIé©±åŠ¨çš„å¤šå‘¨æœŸæƒé‡ä¼˜åŒ–æ¨¡å‹ï¼Œå®ç°ï¼š
- **ä¸»ç›®æ ‡**ï¼šé€‰å‡ºçš„å¼ºåŠ¿è‚¡åœ¨æœªæ¥20å¤©å†…**ç¡®å®šæ€§æˆ˜èƒœæŒ‡æ•°**
- **æ¬¡ç›®æ ‡**ï¼šåœ¨æ»¡è¶³ä¸»ç›®æ ‡çš„å‰æä¸‹ï¼Œ**å›æŠ¥ç‡è¶Šé«˜è¶Šå¥½**
- **è¯„ä¼°æ ‡å‡†**ï¼š
  - èƒœç‡ï¼ˆæˆ˜èƒœæŒ‡æ•°çš„æ¦‚ç‡ï¼‰â‰¥ 70%
  - è¶…é¢æ”¶ç›Šç‡ï¼ˆç›¸å¯¹æŒ‡æ•°ï¼‰æœ€å¤§åŒ–
  - æœ€å¤§å›æ’¤æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…

---

## äºŒã€æ•°æ®å‡†å¤‡æ–¹æ¡ˆ

### 2.1 å¤šå‘¨æœŸæ¶¨å¹…æ•°æ®æ”¶é›†

éœ€è¦æ”¶é›†**1å¤©è‡³20å¤©**å„å‘¨æœŸçš„æ¶¨å¹…å‰100å¼ºåŠ¿è‚¡æ•°æ®ï¼š

```python
# æ•°æ®ç»“æ„ç¤ºä¾‹
multi_period_data = {
    'date': '2024-01-15',
    'period_1': ['SZSE.159915', 'SZSE.159995', ...],  # 1æ—¥æ¶¨å¹…å‰100
    'period_2': ['SZSE.159915', 'SHSE.510300', ...],  # 2æ—¥æ¶¨å¹…å‰100
    'period_3': [...],                                 # 3æ—¥æ¶¨å¹…å‰100
    ...
    'period_20': [...],                                # 20æ—¥æ¶¨å¹…å‰100
    'future_20d_return': {                            # æœªæ¥20å¤©å®é™…æ”¶ç›Š
        'SZSE.159915': 0.085,
        'SZSE.159995': 0.062,
        ...
    },
    'benchmark_return': 0.035                         # æŒ‡æ•°20æ—¥æ”¶ç›Š
}
```

### 2.2 ç‰¹å¾å·¥ç¨‹

ä¸ºæ¯åªè‚¡ç¥¨æ„å»ºç‰¹å¾å‘é‡ï¼š

| ç‰¹å¾ç»´åº¦ | è¯´æ˜ | ç¤ºä¾‹ |
|---------|------|------|
| `rank_1d` | åœ¨1æ—¥æ¶¨å¹…æ¦œçš„æ’åï¼ˆ1-100ï¼‰ | 5 |
| `rank_2d` | åœ¨2æ—¥æ¶¨å¹…æ¦œçš„æ’åï¼ˆ1-100ï¼‰ | 12 |
| ... | ... | ... |
| `rank_20d` | åœ¨20æ—¥æ¶¨å¹…æ¦œçš„æ’åï¼ˆ1-100ï¼‰ | 3 |
| `in_period_k` | æ˜¯å¦å‡ºç°åœ¨ç¬¬kå¤©æ¦œå•ä¸­ï¼ˆ0/1ç¼–ç ï¼‰ | [1,1,1,0,...,1] |
| `consistency_score` | å¤šå‘¨æœŸä¸€è‡´æ€§å¾—åˆ†ï¼ˆå‡ºç°é¢‘æ¬¡ï¼‰ | 15/20 |
| `momentum_decay` | åŠ¨é‡è¡°å‡ç³»æ•°ï¼ˆè¿‘æœŸå‘¨æœŸæƒé‡æ›´é«˜ï¼‰ | - |

### 2.3 æ ‡ç­¾ç”Ÿæˆ

```python
# å¯¹äºæ¯åªè‚¡ç¥¨ï¼Œç”ŸæˆäºŒåˆ†ç±» + å›å½’æ ‡ç­¾
label = {
    'beat_index': 1 if stock_return > benchmark_return else 0,  # æ˜¯å¦è·‘èµ¢
    'excess_return': stock_return - benchmark_return,           # è¶…é¢æ”¶ç›Š
    'future_20d_return': 0.085                                  # æœªæ¥20æ—¥ç»å¯¹æ”¶ç›Š
}
```

### 2.4 æ ·æœ¬æ„å»º

- **æ—¶é—´çª—å£**ï¼š2020-01-01 è‡³ 2024-12-31ï¼ˆçº¦5å¹´æ•°æ®ï¼‰
- **è®­ç»ƒé›†**ï¼š2020-01-01 è‡³ 2023-12-31ï¼ˆ80%ï¼‰
- **éªŒè¯é›†**ï¼š2024-01-01 è‡³ 2024-06-30ï¼ˆ10%ï¼‰
- **æµ‹è¯•é›†**ï¼š2024-07-01 è‡³ 2024-12-31ï¼ˆ10%ï¼‰
- **æ ·æœ¬æ€»é‡**ï¼šçº¦ 100è‚¡/å¤© Ã— 1000äº¤æ˜“æ—¥ = 10ä¸‡æ¡æ ·æœ¬

---

## ä¸‰ã€AIæ¨¡å‹è®¾è®¡æ–¹æ¡ˆ

### 3.1 æ¨¡å‹æ¶æ„ï¼šåŒé˜¶æ®µä¼˜åŒ–

#### ç¬¬ä¸€é˜¶æ®µï¼šæƒé‡å­¦ä¹ æ¨¡å‹ï¼ˆWeight Learning Modelï¼‰

**ç›®æ ‡**ï¼šå­¦ä¹ 1-20å¤©å„å‘¨æœŸçš„æœ€ä¼˜æƒé‡ç³»æ•°

**æ¨¡å‹é€‰æ‹©**ï¼š
- **æ–¹æ¡ˆA**ï¼šçº¿æ€§å›å½’ + L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰
  ```python
  # ä¼˜åŠ¿ï¼šå¯è§£é‡Šæ€§å¼ºï¼Œæƒé‡ç›´è§‚
  score = w1*rank_1d + w2*rank_2d + ... + w20*rank_20d
  ```
- **æ–¹æ¡ˆB**ï¼šæ¢¯åº¦æå‡æ ‘ï¼ˆLightGBM/XGBoostï¼‰
  ```python
  # ä¼˜åŠ¿ï¼šè‡ªåŠ¨æ•æ‰éçº¿æ€§å…³ç³»
  feature_importance = model.feature_importances_
  ```
- **æ–¹æ¡ˆC**ï¼šç¥ç»ç½‘ç»œï¼ˆå¯é€‰ï¼‰
  ```python
  # ä¼˜åŠ¿ï¼šå­¦ä¹ å¤æ‚äº¤äº’ï¼Œä½†å¯è§£é‡Šæ€§å¼±
  Input(20) -> Dense(64) -> Dense(32) -> Output(1)
  ```

**æ¨èæ–¹æ¡ˆBï¼ˆLightGBMï¼‰** - å¹³è¡¡æ€§èƒ½ä¸å¯è§£é‡Šæ€§

#### ç¬¬äºŒé˜¶æ®µï¼šè‚¡ç¥¨é€‰æ‹©æ¨¡å‹ï¼ˆStock Selection Modelï¼‰

**ç›®æ ‡**ï¼šåŸºäºå­¦ä¹ åˆ°çš„æƒé‡ï¼Œé¢„æµ‹æœªæ¥20å¤©è¡¨ç°

**æŸå¤±å‡½æ•°è®¾è®¡**ï¼š
```python
# å¤šç›®æ ‡æŸå¤±å‡½æ•°
loss = Î± * classification_loss + Î² * ranking_loss + Î³ * regression_loss

# å…¶ä¸­ï¼š
# classification_loss: æ˜¯å¦è·‘èµ¢æŒ‡æ•°ï¼ˆäº¤å‰ç†µï¼‰
# ranking_loss: æ’åºæŸå¤±ï¼ˆListNet/LambdaRankï¼‰
# regression_loss: è¶…é¢æ”¶ç›Šé¢„æµ‹ï¼ˆMSEï¼‰

# å‚æ•°è®¾ç½®ï¼š
Î± = 0.5  # ä¼˜å…ˆä¿è¯èƒœç‡
Î² = 0.3  # å…¶æ¬¡ä¼˜åŒ–æ’åº
Î³ = 0.2  # æœ€åä¼˜åŒ–ç»å¯¹æ”¶ç›Š
```

### 3.2 æƒé‡çº¦æŸæ¡ä»¶

```python
constraints = {
    'weight_sum': sum(weights) == 1.0,           # æƒé‡å½’ä¸€åŒ–
    'weight_range': -1.0 â‰¤ wi â‰¤ 1.0,             # å…è®¸è´Ÿæƒé‡ï¼ˆåè½¬å› å­ï¼‰
    'sparsity': L1_penalty,                       # ç¨€ç–æ€§çº¦æŸï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
    'monotonicity': w[é•¿æœŸ] > w[çŸ­æœŸ] (å¯é€‰)      # åŠ¨é‡å•è°ƒæ€§
}
```

### 3.3 è®­ç»ƒæµç¨‹

```mermaid
graph LR
A[å†å²æ•°æ®] --> B[ç‰¹å¾å·¥ç¨‹]
B --> C[åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†]
C --> D[æ¨¡å‹è®­ç»ƒ]
D --> E[è¶…å‚æ•°è°ƒä¼˜]
E --> F[äº¤å‰éªŒè¯]
F --> G{æ»¡è¶³ç›®æ ‡?}
G -->|å¦| D
G -->|æ˜¯| H[æµ‹è¯•é›†éªŒè¯]
H --> I[éƒ¨ç½²åº”ç”¨]
```

---

## å››ã€è¯„ä¼°æŒ‡æ ‡ä½“ç³»

### 4.1 æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡åç§° | è®¡ç®—å…¬å¼ | ç›®æ ‡å€¼ |
|---------|---------|--------|
| **èƒœç‡** | P(é€‰ä¸­è‚¡ç¥¨æ”¶ç›Š > æŒ‡æ•°æ”¶ç›Š) | â‰¥ 70% |
| **å¹³å‡è¶…é¢æ”¶ç›Š** | Mean(è‚¡ç¥¨æ”¶ç›Š - æŒ‡æ•°æ”¶ç›Š) | â‰¥ 5% |
| **ä¿¡æ¯æ¯”ç‡ (IR)** | è¶…é¢æ”¶ç›Šå‡å€¼ / è¶…é¢æ”¶ç›Šæ ‡å‡†å·® | â‰¥ 1.5 |
| **æœ€å¤§å›æ’¤** | Max(å³°å€¼ - è°·å€¼) / å³°å€¼ | â‰¤ 15% |
| **å¤æ™®æ¯”ç‡** | (å¹´åŒ–æ”¶ç›Š - æ— é£é™©åˆ©ç‡) / å¹´åŒ–æ³¢åŠ¨ç‡ | â‰¥ 2.0 |

### 4.2 é£æ§æŒ‡æ ‡

```python
risk_metrics = {
    'var_95': '95%ç½®ä¿¡æ°´å¹³çš„é£é™©ä»·å€¼',
    'tail_risk': 'æç«¯æƒ…å†µä¸‹çš„å°¾éƒ¨é£é™©',
    'correlation_with_index': 'ä¸æŒ‡æ•°çš„ç›¸å…³ç³»æ•°ï¼ˆç›®æ ‡<0.8ï¼‰',
    'turnover_rate': 'æ¢æ‰‹ç‡ï¼ˆç›®æ ‡<30%/æœˆï¼‰'
}
```

---

## äº”ã€å®æ–½æ­¥éª¤

### é˜¶æ®µä¸€ï¼šæ•°æ®å‡†å¤‡ï¼ˆ2-3å‘¨ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
1. [ ] ç¼–å†™å¤šå‘¨æœŸæ¶¨å¹…è®¡ç®—æ¨¡å—
   ```python
   def calculate_multi_period_gains(prices_df, periods=[1,2,...,20]):
       """è®¡ç®—1-20å¤©å„å‘¨æœŸçš„æ¶¨å¹…æ’å"""
       pass
   ```

2. [ ] æ„å»ºæ ‡ç­¾ç”Ÿæˆå™¨
   ```python
   def generate_labels(stock, date, horizon=20):
       """ç”Ÿæˆæœªæ¥20å¤©çš„è¡¨ç°æ ‡ç­¾"""
       pass
   ```

3. [ ] æ•°æ®è´¨é‡æ£€æŸ¥
   - ç¼ºå¤±å€¼å¤„ç†
   - å¼‚å¸¸å€¼æ£€æµ‹
   - ç”Ÿå­˜åå·®ä¿®æ­£

### é˜¶æ®µäºŒï¼šæ¨¡å‹å¼€å‘ï¼ˆ3-4å‘¨ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
1. [ ] åŸºçº¿æ¨¡å‹ï¼ˆç­‰æƒé‡ï¼‰
   ```python
   baseline_weights = [1/20] * 20  # æ‰€æœ‰å‘¨æœŸç­‰æƒé‡
   ```

2. [ ] LightGBMæ¨¡å‹è®­ç»ƒ
   ```python
   model = lgb.LGBMRanker(
       objective='lambdarank',
       metric='ndcg',
       num_leaves=31,
       learning_rate=0.05
   )
   ```

3. [ ] è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptuna/GridSearchï¼‰
   ```python
   study = optuna.create_study(direction='maximize')
   study.optimize(objective, n_trials=100)
   ```

4. [ ] æ¨¡å‹è§£é‡Šï¼ˆSHAPå€¼åˆ†æï¼‰
   ```python
   shap_values = shap.TreeExplainer(model).shap_values(X_test)
   shap.summary_plot(shap_values, X_test)
   ```

### é˜¶æ®µä¸‰ï¼šå›æµ‹éªŒè¯ï¼ˆ2å‘¨ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
1. [ ] æ»šåŠ¨çª—å£å›æµ‹
   ```python
   for train_end in rolling_windows:
       model.fit(train_data)
       predictions = model.predict(test_data)
       evaluate_performance(predictions)
   ```

2. [ ] å¯¹æ¯”å®éªŒ
   - å½“å‰å›ºå®šæƒé‡ç­–ç•¥ vs AIä¼˜åŒ–æƒé‡
   - ä¸åŒæ¨¡å‹æ¶æ„å¯¹æ¯”
   - æ¶ˆèå®éªŒï¼ˆablation studyï¼‰

3. [ ] ç¨³å¥æ€§æµ‹è¯•
   - ä¸åŒå¸‚åœºç¯å¢ƒï¼ˆç‰›å¸‚/ç†Šå¸‚/éœ‡è¡ï¼‰
   - æç«¯è¡Œæƒ…å‹åŠ›æµ‹è¯•

### é˜¶æ®µå››ï¼šé›†æˆéƒ¨ç½²ï¼ˆ1-2å‘¨ï¼‰

**ä»»åŠ¡æ¸…å•**ï¼š
1. [ ] ä¿®æ”¹ `core/signal.py`
   ```python
   # æ›¿æ¢å›ºå®šæƒé‡ä¸ºAIæ¨¡å‹è¾“å‡º
   def get_ranking_ai(context, current_dt):
       features = extract_multi_period_features(context, current_dt)
       scores = ai_model.predict(features)
       return scores
   ```

2. [ ] æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
   ```python
   # ä¿å­˜æ¨¡å‹
   joblib.dump(model, f'models/weight_model_v{version}.pkl')
   ```

3. [ ] å®ç›˜A/Bæµ‹è¯•
   - 70%èµ„é‡‘ä½¿ç”¨AIæ¨¡å‹
   - 30%èµ„é‡‘ä½¿ç”¨åŸæœ‰ç­–ç•¥
   - å¯¹æ¯”7å¤©åæ•ˆæœ

---

## å…­ã€æŠ€æœ¯æ¶æ„è®¾è®¡

### 6.1 ç³»ç»Ÿæ¨¡å—åˆ’åˆ†

```
project_root/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ signal.py                # ä¿¡å·ç”Ÿæˆï¼ˆé›†æˆAIæ¨¡å‹ï¼‰
â”‚   â”œâ”€â”€ strategy.py              # ç­–ç•¥æ‰§è¡Œ
â”‚   â””â”€â”€ ai_scorer.py             # ã€æ–°å¢ã€‘AIè¯„åˆ†æ¨¡å—
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ weight_optimizer.py      # ã€æ–°å¢ã€‘æƒé‡å­¦ä¹ æ¨¡å‹
â”‚   â”œâ”€â”€ stock_selector.py        # ã€æ–°å¢ã€‘è‚¡ç¥¨é€‰æ‹©æ¨¡å‹
â”‚   â””â”€â”€ trained_models/          # è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ feature_engineering.py   # ã€æ–°å¢ã€‘ç‰¹å¾å·¥ç¨‹
â”‚   â”œâ”€â”€ label_generator.py       # ã€æ–°å¢ã€‘æ ‡ç­¾ç”Ÿæˆ
â”‚   â””â”€â”€ multi_period_data.py     # ã€æ–°å¢ã€‘å¤šå‘¨æœŸæ•°æ®æ”¶é›†
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ backtester.py            # ã€æ–°å¢ã€‘å›æµ‹æ¡†æ¶
â”‚   â”œâ”€â”€ metrics.py               # ã€æ–°å¢ã€‘è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ visualizer.py            # ã€æ–°å¢ã€‘å¯è§†åŒ–
â””â”€â”€ notebooks/
    â”œâ”€â”€ 01_data_exploration.ipynb
    â”œâ”€â”€ 02_model_training.ipynb
    â””â”€â”€ 03_results_analysis.ipynb
```

### 6.2 æ ¸å¿ƒä»£ç æ¡†æ¶

#### æ•°æ®å±‚ï¼ˆdata/multi_period_data.pyï¼‰
```python
class MultiPeriodDataCollector:
    """å¤šå‘¨æœŸæ¶¨å¹…æ•°æ®æ”¶é›†å™¨"""

    def __init__(self, periods=range(1, 21)):
        self.periods = periods

    def collect_top_performers(self, date, top_n=100):
        """
        æ”¶é›†æŒ‡å®šæ—¥æœŸå„å‘¨æœŸçš„æ¶¨å¹…å‰100è‚¡ç¥¨

        Returns:
            {
                'period_1': [(symbol, gain), ...],
                'period_2': [(symbol, gain), ...],
                ...
            }
        """
        pass

    def generate_features(self, stock, date):
        """
        ä¸ºå•åªè‚¡ç¥¨ç”Ÿæˆç‰¹å¾å‘é‡

        Returns:
            [rank_1d, rank_2d, ..., rank_20d, consistency, ...]
        """
        pass

    def generate_labels(self, stock, date, horizon=20):
        """
        ç”Ÿæˆæœªæ¥20å¤©çš„æ ‡ç­¾

        Returns:
            {
                'beat_index': bool,
                'excess_return': float,
                'future_return': float
            }
        """
        pass
```

#### æ¨¡å‹å±‚ï¼ˆmodels/weight_optimizer.pyï¼‰
```python
import lightgbm as lgb
from sklearn.model_selection import TimeSeriesSplit

class WeightOptimizer:
    """å¤šå‘¨æœŸæƒé‡ä¼˜åŒ–å™¨"""

    def __init__(self, objective='lambdarank'):
        self.objective = objective
        self.model = None
        self.best_weights = None

    def fit(self, X_train, y_train, X_val, y_val):
        """
        è®­ç»ƒæƒé‡å­¦ä¹ æ¨¡å‹

        Args:
            X_train: shape (N, 20) - å„å‘¨æœŸæ’åç‰¹å¾
            y_train: shape (N,) - æ˜¯å¦è·‘èµ¢æŒ‡æ•°ï¼ˆ0/1ï¼‰
        """
        params = {
            'objective': self.objective,
            'metric': ['auc', 'binary_logloss'],
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'lambda_l1': 0.1,  # L1æ­£åˆ™åŒ–ï¼Œäº§ç”Ÿç¨€ç–æƒé‡
            'verbosity': -1
        }

        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

        self.model = lgb.train(
            params,
            train_data,
            num_boost_round=1000,
            valid_sets=[train_data, val_data],
            callbacks=[lgb.early_stopping(stopping_rounds=50)]
        )

        # æå–ç‰¹å¾é‡è¦æ€§ä½œä¸ºæƒé‡
        self.best_weights = self.model.feature_importance(importance_type='gain')
        self.best_weights = self.best_weights / self.best_weights.sum()  # å½’ä¸€åŒ–

        return self

    def predict_score(self, features):
        """
        ä¸ºæ–°è‚¡ç¥¨è®¡ç®—ç»¼åˆè¯„åˆ†

        Args:
            features: shape (20,) - å„å‘¨æœŸæ’å

        Returns:
            score: float - ç»¼åˆè¯„åˆ†
        """
        if self.model is None:
            raise ValueError("Model not trained yet!")

        return self.model.predict([features])[0]

    def get_weights(self):
        """è¿”å›å­¦ä¹ åˆ°çš„æƒé‡å‘é‡"""
        return {
            f'period_{i+1}d': w
            for i, w in enumerate(self.best_weights)
        }
```

#### è¯„åˆ†æ¨¡å—ï¼ˆcore/ai_scorer.pyï¼‰
```python
import joblib
from pathlib import Path

class AIScorer:
    """AIé©±åŠ¨çš„è¯„åˆ†ç³»ç»Ÿ"""

    def __init__(self, model_path='models/trained_models/latest.pkl'):
        self.model = joblib.load(model_path)
        self.periods = range(1, 21)

    def score_stocks(self, context, current_dt, whitelist):
        """
        ä¸ºç™½åå•è‚¡ç¥¨è®¡ç®—AIè¯„åˆ†

        Args:
            context: å›æµ‹/å®ç›˜ä¸Šä¸‹æ–‡
            current_dt: å½“å‰æ—¥æœŸ
            whitelist: å€™é€‰è‚¡ç¥¨åˆ—è¡¨

        Returns:
            {symbol: score} çš„å­—å…¸
        """
        from data.multi_period_data import MultiPeriodDataCollector

        collector = MultiPeriodDataCollector(periods=self.periods)
        scores = {}

        for symbol in whitelist:
            try:
                # æå–ç‰¹å¾
                features = collector.generate_features(symbol, current_dt)

                # AIé¢„æµ‹
                score = self.model.predict_score(features)
                scores[symbol] = score

            except Exception as e:
                logger.warning(f"Failed to score {symbol}: {e}")
                scores[symbol] = 0

        return scores
```

#### é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿï¼ˆcore/signal.pyï¼‰
```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ 
from core.ai_scorer import AIScorer

# ä¿®æ”¹ get_ranking å‡½æ•°
def get_ranking(context, current_dt, whitelist):
    """
    è·å–è‚¡ç¥¨æ’åï¼ˆAIå¢å¼ºç‰ˆï¼‰
    """
    # å¯é€‰ï¼šé€šè¿‡é…ç½®åˆ‡æ¢AIæ¨¡å¼æˆ–ä¼ ç»Ÿæ¨¡å¼
    if config.USE_AI_SCORING:
        logger.info("ğŸ¤– Using AI-based scoring...")
        ai_scorer = AIScorer(model_path=config.AI_MODEL_PATH)
        scores = ai_scorer.score_stocks(context, current_dt, whitelist)

        # è½¬æ¢ä¸ºæ’åæ ¼å¼
        ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return pd.DataFrame(ranked, columns=['symbol', 'score'])

    else:
        # ä¿ç•™åŸæœ‰çš„å›ºå®šæƒé‡é€»è¾‘
        logger.info("ğŸ“Š Using traditional scoring...")
        # ... åŸæœ‰ä»£ç  ...
```

---

## ä¸ƒã€é¢„æœŸæˆæœä¸é£é™©

### 7.1 é¢„æœŸæˆæœ

**é‡åŒ–æŒ‡æ ‡**ï¼š
- èƒœç‡ï¼šä»å½“å‰ ~60% æå‡è‡³ **75%Â±5%**
- è¶…é¢æ”¶ç›Šï¼šä» 3-4% æå‡è‡³ **6-8%**
- ä¿¡æ¯æ¯”ç‡ï¼šä» 1.2 æå‡è‡³ **1.8-2.0**
- æœ€å¤§å›æ’¤ï¼šæ§åˆ¶åœ¨ **12%ä»¥å†…**ï¼ˆå½“å‰ ~15%ï¼‰

**ä¸šåŠ¡ä»·å€¼**ï¼š
1. **æ•°æ®é©±åŠ¨å†³ç­–**ï¼šæƒé‡å‚æ•°ç”±AIè‡ªåŠ¨ä¼˜åŒ–ï¼Œæ— éœ€äººå·¥è°ƒå‚
2. **è‡ªé€‚åº”æ€§**ï¼šæ¨¡å‹å¯å®šæœŸé‡è®­ç»ƒï¼Œé€‚åº”å¸‚åœºç¯å¢ƒå˜åŒ–
3. **å¯è§£é‡Šæ€§**ï¼šé€šè¿‡SHAPåˆ†æç†è§£å„å‘¨æœŸçš„è´¡çŒ®åº¦
4. **ç³»ç»ŸåŒ–**ï¼šå»ºç«‹å®Œæ•´çš„AIé€‰è‚¡å·¥ä½œæµ

### 7.2 é£é™©ä¸åº”å¯¹

| é£é™© | æè¿° | åº”å¯¹æªæ–½ |
|-----|------|---------|
| **è¿‡æ‹Ÿåˆé£é™©** | æ¨¡å‹åœ¨è®­ç»ƒé›†è¡¨ç°å¥½ï¼Œä½†æµ‹è¯•é›†å·® | 1. ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯<br>2. L1/L2æ­£åˆ™åŒ–<br>3. ç®€å•æ¨¡å‹ä¼˜å…ˆ |
| **æ•°æ®è´¨é‡** | å†å²æ•°æ®ç¼ºå¤±æˆ–ä¸å‡†ç¡® | 1. æ•°æ®æ¸…æ´—æµç¨‹<br>2. å¤šæ•°æ®æºäº¤å‰éªŒè¯ |
| **å¸‚åœºç¯å¢ƒå˜åŒ–** | 2020-2024çš„è§„å¾‹ä¸é€‚ç”¨äºæœªæ¥ | 1. å®šæœŸé‡è®­ç»ƒï¼ˆæ¯å­£åº¦ï¼‰<br>2. åœ¨çº¿å­¦ä¹ æœºåˆ¶<br>3. ä¿ç•™äººå·¥å¹²é¢„æ¥å£ |
| **æŠ€æœ¯å€ºåŠ¡** | AIç³»ç»Ÿå¢åŠ ç»´æŠ¤å¤æ‚åº¦ | 1. å®Œå–„æ–‡æ¡£<br>2. å•å…ƒæµ‹è¯•è¦†ç›–<br>3. A/Bæµ‹è¯•ç°åº¦å‘å¸ƒ |

---

## å…«ã€é…ç½®æ–‡ä»¶æ‰©å±•

åœ¨ `config.py` ä¸­æ–°å¢AIç›¸å…³é…ç½®ï¼š

```python
# === AIæ¨¡å‹é…ç½® ===
USE_AI_SCORING = True  # æ˜¯å¦å¯ç”¨AIè¯„åˆ†ï¼ˆFalseåˆ™ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼‰
AI_MODEL_PATH = 'models/trained_models/weight_optimizer_v1.0.pkl'
AI_RETRAIN_INTERVAL = 90  # æ¨¡å‹é‡è®­ç»ƒå‘¨æœŸï¼ˆå¤©ï¼‰

# ç‰¹å¾å·¥ç¨‹
MULTI_PERIOD_RANGE = list(range(1, 21))  # ä½¿ç”¨1-20å¤©å‘¨æœŸ
TOP_N_PER_PERIOD = 100  # æ¯ä¸ªå‘¨æœŸå–å‰100å

# æ¨¡å‹è®­ç»ƒ
TRAIN_START_DATE = '2020-01-01'
TRAIN_END_DATE = '2023-12-31'
VAL_START_DATE = '2024-01-01'
VAL_END_DATE = '2024-06-30'

# ä¼˜åŒ–ç›®æ ‡
TARGET_WIN_RATE = 0.70  # ç›®æ ‡èƒœç‡
TARGET_EXCESS_RETURN = 0.05  # ç›®æ ‡è¶…é¢æ”¶ç›Š
MAX_DRAWDOWN_TOLERANCE = 0.15  # æœ€å¤§å›æ’¤å®¹å¿åº¦

# æŸå¤±å‡½æ•°æƒé‡
ALPHA_CLASSIFICATION = 0.5  # èƒœç‡æƒé‡
BETA_RANKING = 0.3  # æ’åºæƒé‡
GAMMA_REGRESSION = 0.2  # æ”¶ç›Šé¢„æµ‹æƒé‡
```

---

## ä¹ã€å¼€å‘æ—¶é—´è¡¨

| é˜¶æ®µ | ä»»åŠ¡ | è´Ÿè´£äºº | æ—¶é•¿ | é‡Œç¨‹ç¢‘ |
|-----|------|--------|------|--------|
| **Phase 1** | æ•°æ®æ”¶é›†ä¸æ¸…æ´— | æ•°æ®å·¥ç¨‹å¸ˆ | 2å‘¨ | å®Œæˆå¤šå‘¨æœŸæ•°æ®é›† |
| **Phase 2** | ç‰¹å¾å·¥ç¨‹ | ç®—æ³•å·¥ç¨‹å¸ˆ | 1å‘¨ | ç‰¹å¾åº“å»ºç«‹ |
| **Phase 3** | æ¨¡å‹è®­ç»ƒ | AIå·¥ç¨‹å¸ˆ | 3å‘¨ | åŸºçº¿æ¨¡å‹è¾¾æ ‡ |
| **Phase 4** | å›æµ‹éªŒè¯ | é‡åŒ–ç ”ç©¶å‘˜ | 2å‘¨ | å›æµ‹æŠ¥å‘Šå®Œæˆ |
| **Phase 5** | ç³»ç»Ÿé›†æˆ | åç«¯å·¥ç¨‹å¸ˆ | 1å‘¨ | ä»£ç åˆå¹¶ä¸Šçº¿ |
| **Phase 6** | å®ç›˜æµ‹è¯• | å…¨å‘˜ | 2å‘¨ | A/Bæµ‹è¯•ç»“è®º |
| **æ€»è®¡** | - | - | **11å‘¨** | æ­£å¼æŠ•äº§ |

---

## åã€å‚è€ƒèµ„æ–™

### 10.1 å­¦æœ¯æ–‡çŒ®
1. **å¤šå› å­é€‰è‚¡**ï¼šFama-French Five-Factor Model
2. **åŠ¨é‡ç­–ç•¥**ï¼šJegadeesh & Titman (1993) "Returns to Buying Winners"
3. **æœºå™¨å­¦ä¹ é‡åŒ–**ï¼šDixon et al. (2020) "Machine Learning in Finance"

### 10.2 æŠ€æœ¯å·¥å…·
- **æ•°æ®å¤„ç†**ï¼šPandas, NumPy
- **æœºå™¨å­¦ä¹ **ï¼šLightGBM, XGBoost, Scikit-learn
- **å›æµ‹æ¡†æ¶**ï¼šæ˜é‡‘é‡åŒ–ï¼ˆå½“å‰ç³»ç»Ÿï¼‰
- **æ¨¡å‹è§£é‡Š**ï¼šSHAP, LIME
- **å®éªŒç®¡ç†**ï¼šMLflow, Weights & Biases

### 10.3 ä»£ç ç¤ºä¾‹åº“
- [LightGBMå®˜æ–¹æ–‡æ¡£](https://lightgbm.readthedocs.io/)
- [Optunaè¶…å‚æ•°ä¼˜åŒ–](https://optuna.org/)
- [SHAPå¯è§£é‡Šæ€§](https://github.com/slundberg/shap)

---

## é™„å½•Aï¼šå¿«é€Ÿå¯åŠ¨æŒ‡å—

### A1. ç¯å¢ƒå‡†å¤‡
```bash
# å®‰è£…ä¾èµ–
pip install lightgbm optuna shap scikit-learn matplotlib seaborn

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p models/trained_models
mkdir -p data/processed
mkdir -p evaluation/reports
```

### A2. æ•°æ®å‡†å¤‡è„šæœ¬
```python
# scripts/prepare_data.py
from data.multi_period_data import MultiPeriodDataCollector

collector = MultiPeriodDataCollector(periods=range(1, 21))
dataset = collector.build_dataset(
    start_date='2020-01-01',
    end_date='2024-12-31',
    save_path='data/processed/training_data.pkl'
)
print(f"Dataset size: {len(dataset)} samples")
```

### A3. æ¨¡å‹è®­ç»ƒè„šæœ¬
```python
# scripts/train_model.py
from models.weight_optimizer import WeightOptimizer
import joblib

# åŠ è½½æ•°æ®
X_train, y_train = load_data('data/processed/training_data.pkl')

# è®­ç»ƒæ¨¡å‹
optimizer = WeightOptimizer()
optimizer.fit(X_train, y_train, X_val, y_val)

# ä¿å­˜æ¨¡å‹
joblib.dump(optimizer, 'models/trained_models/weight_optimizer_v1.0.pkl')

# è¾“å‡ºæƒé‡
print("Learned weights:")
for period, weight in optimizer.get_weights().items():
    print(f"{period}: {weight:.4f}")
```

### A4. å›æµ‹éªŒè¯è„šæœ¬
```python
# scripts/backtest_ai_model.py
from evaluation.backtester import Backtester

backtester = Backtester(
    start_date='2024-01-01',
    end_date='2024-12-31',
    model_path='models/trained_models/weight_optimizer_v1.0.pkl'
)

results = backtester.run()
backtester.plot_results(save_path='evaluation/reports/backtest_results.png')

print(f"Win Rate: {results['win_rate']:.2%}")
print(f"Excess Return: {results['excess_return']:.2%}")
print(f"Sharpe Ratio: {results['sharpe_ratio']:.2f}")
```

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv1.0
**åˆ›å»ºæ—¥æœŸ**ï¼š2024-02-07
**ç»´æŠ¤è€…**ï¼šé‡åŒ–å›¢é˜Ÿ
**æœ€åæ›´æ–°**ï¼š2024-02-07
